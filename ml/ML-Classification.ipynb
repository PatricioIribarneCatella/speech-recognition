{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from numpy.linalg import eig, norm, det\n",
    "from scipy.linalg import inv\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ellipse(sigma, mean, scale=1):\n",
    "    \n",
    "    d, v = eig(inv(sigma))\n",
    "    mat = v @ inv(np.sqrt(np.diag(d)))\n",
    "\n",
    "    N = 200\n",
    "    t = np.arange(0, N) * (2*np.pi) / N\n",
    "\n",
    "    Y1 = scale * np.cos(t)\n",
    "    Y2 = scale * np.sin(t)\n",
    "   \n",
    "    Y = np.array([Y1, Y2])\n",
    "\n",
    "    X = mat.dot(Y)\n",
    "\n",
    "    X1 = X[0]\n",
    "    X2 = X[1]\n",
    "\n",
    "    # move ellipse to mean_k\n",
    "    X1 = X1 + mean[0]\n",
    "    X2 = X2 + mean[1]\n",
    "    \n",
    "    return X1, X2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(d1, d2, d3):\n",
    "\n",
    "    # parse data\n",
    "    with open(d1) as f:\n",
    "        lines_a = list(csv.reader(f, delimiter='\\t'))\n",
    "\n",
    "    with open(d2) as f:\n",
    "        lines_o = list(csv.reader(f, delimiter='\\t'))\n",
    "\n",
    "    with open(d3) as f:\n",
    "        lines_u = list(csv.reader(f, delimiter='\\t'))\n",
    "\n",
    "    lines_a = list(map(lambda x: [int(x[0]), int(x[1])],lines_a))\n",
    "    lines_o = list(map(lambda x: [int(x[0]), int(x[1])],lines_o))\n",
    "    lines_u = list(map(lambda x: [int(x[0]), int(x[1])],lines_u))\n",
    "\n",
    "    # make a 'shuffle' of the samples\n",
    "    np.random.shuffle(lines_a)\n",
    "    np.random.shuffle(lines_o)\n",
    "    np.random.shuffle(lines_u)\n",
    "    \n",
    "    # separate 'train' and 'test' datasets\n",
    "    train_a = np.array(lines_a[:35])\n",
    "    test_a = np.array(lines_a[35:])\n",
    "    \n",
    "    train_o = np.array(lines_o[:35])\n",
    "    test_o = np.array(lines_o[35:])\n",
    "    \n",
    "    train_u = np.array(lines_u[:35])\n",
    "    test_u = np.array(lines_u[35:])\n",
    "    \n",
    "    samples = np.array(list(train_a) + list(train_o) + list(train_u))\n",
    "    \n",
    "    data = {\n",
    "        \"train\": {\n",
    "            \"1\": train_a,\n",
    "            \"2\": train_o,\n",
    "            \"3\": train_u\n",
    "        },\n",
    "        \"test\": {\n",
    "            \"1\": test_a,\n",
    "            \"2\": test_o,\n",
    "            \"3\": test_u\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return samples, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_positive(test, g1, g2, g3, which):\n",
    "\n",
    "    positive = 0\n",
    "\n",
    "    for sample in test:\n",
    "        \n",
    "        predictions = [(g1(sample), \"1\"), (g2(sample), \"2\"), (g3(sample), \"3\")]\n",
    "        prob, k = max(predictions, key=lambda x: x[0])\n",
    "        \n",
    "        if k == which:\n",
    "            positive += 1\n",
    "\n",
    "    return positive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algoritmos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _LDA_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lda(datasets):\n",
    "    \n",
    "    train = datasets[\"train\"]\n",
    "    \n",
    "    # mean calc\n",
    "    means = [np.mean(train[\"1\"], axis=0),\n",
    "             np.mean(train[\"2\"], axis=0),\n",
    "             np.mean(train[\"3\"], axis=0)]\n",
    "    \n",
    "    # sigma calc\n",
    "    sigmas = [np.cov(train[\"1\"], rowvar=False),\n",
    "              np.cov(train[\"2\"], rowvar=False),\n",
    "              np.cov(train[\"3\"], rowvar=False)]\n",
    "\n",
    "    return means, sigmas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _K-Means_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_dist(samples, classes, means):\n",
    "    \n",
    "    dist = 0.0\n",
    "    \n",
    "    for i in range(len(samples)):\n",
    "        d = norm(samples[i] - means[int(classes[i])])\n",
    "        dist += d\n",
    "    \n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_means(samples, means):\n",
    "    \n",
    "    it = 0\n",
    "    classes = np.zeros(len(samples))\n",
    "    dist = np.zeros(2)\n",
    "    \n",
    "    dist[0] = 0.0\n",
    "    dist[1] = 1E6\n",
    "    \n",
    "    while abs(dist[1] - dist[0]) > 0.01:\n",
    "    \n",
    "        # calculate distance from every point\n",
    "        # to the centroids (a.k.a means) and find a class for\n",
    "        # each point\n",
    "        for i in range(len(samples)):\n",
    "            norms = [(norm(samples[i] - means[k]), k) for k in range(3)]\n",
    "            n, k = min(norms, key=lambda x: x[0])\n",
    "            classes[i] = k\n",
    "\n",
    "        # calculate the new values for the\n",
    "        # centroids (a.k.a means)\n",
    "        for k in range(3):\n",
    "            points_with_class_k = list(map(lambda y: y[0], filter(lambda x: x[1] == k, zip(samples, classes))))\n",
    "            mean_k = np.mean(points_with_class_k, axis=0)\n",
    "            means[k] = mean_k\n",
    "        \n",
    "        # calculate distorsion\n",
    "        dist[0] = dist[1]\n",
    "        dist[1] = calc_dist(samples, classes, means)\n",
    "        it += 1\n",
    "\n",
    "    return means, classes, it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_k_clusters(samples, classes, K=3):\n",
    "    \n",
    "    return [np.array(list(map(lambda y: y[0], filter(lambda x: x[1] == k, zip(samples, classes)))))\n",
    "                    for k in range(K)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_k_sigmas(clusters):\n",
    "    \n",
    "    return np.array([np.cov(c, rowvar=False) for c in clusters])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _EM_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_L(samples, means, sigmas, pi):\n",
    "    \n",
    "    x = samples\n",
    "    \n",
    "    return sum([np.log(sum([multivariate_normal.pdf(x[n], mean=means[k], cov=sigmas[k])*pi[k] for k in range(3)])) for n in range(len(samples))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_means(samples, gammas):\n",
    "\n",
    "    # calc denominator\n",
    "    den = calc_denom(gammas)\n",
    "    \n",
    "    m = np.array([sum([samples[n]*gammas[n][k] for n in range(len(samples))]) for k in range(3)])\n",
    "    \n",
    "    return np.array([m[i]/den[i] for i in range(len(m))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_sigmas(samples, means, gammas):\n",
    "\n",
    "    # calc denominator\n",
    "    den = calc_denom(gammas)\n",
    "    \n",
    "    sigmas = []\n",
    "    sigmas_k = []\n",
    "    \n",
    "    for k in range(3):\n",
    "        for n in range(len(samples)):\n",
    "            res = np.array(list(map(lambda y: [y], samples[n]-means[k])))\n",
    "            s = gammas[n][k] * res @ res.T\n",
    "            sigmas_k.append(s)\n",
    "        sigmas.append(sum(sigmas_k) / den[k])\n",
    "        sigmas_k.clear()\n",
    "    \n",
    "    return sigmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_pi(gammas, N):\n",
    "\n",
    "    num = calc_denom(gammas)\n",
    "    \n",
    "    return num / N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculates the sum of gammas\n",
    "# for all the classses\n",
    "# returns:\n",
    "#   [sum(gammas[0]), sum(gammas[1]), sum(gammas[2])]\n",
    "def calc_denom(gammas):\n",
    "\n",
    "    N = len(gammas)\n",
    "    \n",
    "    d = np.array([sum([gammas[n][k] for n in range(N)]) for k in range(3)])\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gamma(sample, means, sigmas, pi):\n",
    "    \n",
    "    nums = np.array([multivariate_normal.pdf(sample, mean=means[k], cov=sigmas[k])*pi[k] for k in range(3)])\n",
    "\n",
    "    den = nums.sum()\n",
    "    \n",
    "    return nums / den"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def em(samples, means, sigmas, pi):\n",
    "\n",
    "    it = 0\n",
    "    L = np.zeros(2)\n",
    "    L[0] = 0.0\n",
    "    L[1] = calc_L(samples, means, sigmas, pi)\n",
    "    deltaL = L[1] - L[0]\n",
    "    \n",
    "    while abs(deltaL) > 0.01:\n",
    "\n",
    "        ############\n",
    "        # 'E' step #\n",
    "        ############\n",
    "        gammas = np.array([gamma(samples[i], means, sigmas, pi) for i in range(len(samples))])\n",
    "\n",
    "        ############\n",
    "        # 'M' step #\n",
    "        ############\n",
    "        \n",
    "        # calc new means\n",
    "        means = calc_means(samples, gammas)\n",
    "\n",
    "        # calc new sigmas\n",
    "        sigmas = calc_sigmas(samples, means, gammas)\n",
    "\n",
    "        # calc new pi\n",
    "        pi = calc_pi(gammas, len(samples))\n",
    "\n",
    "        L[0] = L[1]\n",
    "        L[1] = calc_L(samples, means, sigmas, pi)\n",
    "        deltaL = L[1] - L[0]\n",
    "        it += 1\n",
    "\n",
    "    return means, sigmas, gammas, pi, it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_em_sigmas(data):\n",
    "\n",
    "    d1 = data[\"1\"]\n",
    "    d2 = data[\"2\"]\n",
    "    d3 = data[\"3\"]\n",
    "    \n",
    "    sigma = (1/3) * (np.cov(d1, rowvar=False) + np.cov(d2, rowvar=False) + np.cov(d2, rowvar=False))\n",
    "    \n",
    "    return np.array([sigma, sigma, sigma])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random _means_ and _sigmas_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_means(train):\n",
    "\n",
    "    train_a = train[\"1\"]\n",
    "    train_o = train[\"2\"]\n",
    "    train_u = train[\"3\"]\n",
    "    \n",
    "    points_a = [train_a[random.randrange(0, len(train_a))] for i in range(5)]\n",
    "    points_o = [train_o[random.randrange(0, len(train_o))] for i in range(5)]\n",
    "    points_u = [train_u[random.randrange(0, len(train_u))] for i in range(5)]\n",
    "\n",
    "    data = {\n",
    "        \"1\": points_a,\n",
    "        \"2\": points_o,\n",
    "        \"3\": points_u\n",
    "    }\n",
    "    \n",
    "    means = [\n",
    "        np.mean(points_a, axis=0),\n",
    "        np.mean(points_o, axis=0),\n",
    "        np.mean(points_u, axis=0)\n",
    "    ]\n",
    "    \n",
    "    return means, data\n",
    "\n",
    "def get_zone_division_means(train):\n",
    "    \n",
    "    train_a = train[\"1\"]\n",
    "    train_o = train[\"2\"]\n",
    "    train_u = train[\"3\"]\n",
    "    \n",
    "    samples = np.array(list(train_a) + list(train_o) + list(train_u))\n",
    "    \n",
    "    mean = np.mean(samples, axis=0)\n",
    "    \n",
    "    samples_moved = np.array(list(map(lambda x: x - mean, samples)))\n",
    "    phases = np.array(list(map(lambda x: np.mod(np.arctan2(x[1], x[0]), 2*np.pi), samples_moved)))\n",
    "    \n",
    "    samples_phases = list(zip(samples, phases))\n",
    "    \n",
    "    sample_phase_1 = list(filter(lambda x: x[1] > 0.0 and x[1] <= (2/3)*np.pi, samples_phases))\n",
    "    sample_phase_2 = list(filter(lambda x: x[1] > (2/3)*np.pi and x[1] <= (4/3)*np.pi, samples_phases))\n",
    "    sample_phase_3 = list(filter(lambda x: x[1] > (4/3)*np.pi and x[1] < 2*np.pi, samples_phases))\n",
    "        \n",
    "    sample_1 = list(map(lambda x: x[0], sample_phase_1))\n",
    "    sample_2 = list(map(lambda x: x[0], sample_phase_2))\n",
    "    sample_3 = list(map(lambda x: x[0], sample_phase_3))\n",
    "\n",
    "    means = [np.mean(sample_1, axis=0), np.mean(sample_2, axis=0), np.mean(sample_3, axis=0)]\n",
    "\n",
    "    data = {\n",
    "        \"1\": sample_1,\n",
    "        \"2\": sample_2,\n",
    "        \"3\": sample_3\n",
    "    }\n",
    "    \n",
    "    return means, data\n",
    "\n",
    "FMEANS = {\n",
    "    \"random\": get_random_means,\n",
    "    \"zone-division\": get_zone_division_means\n",
    "}\n",
    "\n",
    "def get_means(train, which):\n",
    "\n",
    "    return FMEANS[which](train) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aprendizaje"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g_lda(x, mean, sigma, w):\n",
    "    \n",
    "    w1 = inv(sigma) @ mean\n",
    "    w0 = np.log(w) - 0.5 * mean @ inv(sigma).T @ mean.T\n",
    "    \n",
    "    return w1 @ x + w0\n",
    "\n",
    "def g(x, mean, sigma, w):\n",
    "\n",
    "    y = x - mean\n",
    "    \n",
    "    return np.log(w) - 0.5 * (np.log(det(sigma)) + y.T @ inv(sigma) @ y)\n",
    "\n",
    "def learn(samples, datasets, classifier, mean_calculus):\n",
    "\n",
    "    train = datasets[\"train\"]\n",
    "    test = datasets[\"test\"]\n",
    "\n",
    "    if classifier == \"lda\":\n",
    "\n",
    "        means, sigmas = lda(datasets)\n",
    "        \n",
    "        x_tot = len(train[\"1\"]) + len(train[\"2\"]) + len(train[\"3\"])\n",
    "\n",
    "        sigma = (sigmas[0] + sigmas[1] + sigmas[2]) / 3\n",
    "        \n",
    "        g1 = lambda x: g_lda(x, means[0], sigma, len(train[\"1\"])/x_tot)\n",
    "        g2 = lambda x: g_lda(x, means[1], sigma, len(train[\"2\"])/x_tot)\n",
    "        g3 = lambda x: g_lda(x, means[2], sigma, len(train[\"3\"])/x_tot)\n",
    "\n",
    "        parameters = {\n",
    "            \"means\": means,\n",
    "            \"sigmas\": sigmas\n",
    "        }\n",
    "\n",
    "        return g1, g2, g3, parameters, []\n",
    "\n",
    "    means, data = get_means(train, mean_calculus)\n",
    "    \n",
    "    mean_1 = np.mean(test[\"1\"], axis=0)\n",
    "    mean_2 = np.mean(test[\"2\"], axis=0)\n",
    "    mean_3 = np.mean(test[\"3\"], axis=0)\n",
    "\n",
    "    if classifier == \"k-means\":\n",
    "\n",
    "        means, classes, it = k_means(samples, means)\n",
    "        \n",
    "        # separate samples in clusters considering\n",
    "        # the classes\n",
    "        clusters = get_k_clusters(samples, classes)\n",
    "\n",
    "        # calculate the sigmas for every class\n",
    "        sigmas = get_k_sigmas(clusters)\n",
    "        \n",
    "        x_tot = len(clusters[0]) + len(clusters[1]) + len(clusters[2])\n",
    "        \n",
    "        class_1 = np.argmin([norm(mean_1 - means[k]) for k in range(3)])\n",
    "        class_2 = np.argmin([norm(mean_2 - means[k]) for k in range(3)])\n",
    "        class_3 = np.argmin([norm(mean_3 - means[k]) for k in range(3)])\n",
    "\n",
    "        # create the mapping\n",
    "        mapping = {\n",
    "            \"1\": class_1,\n",
    "            \"2\": class_2,\n",
    "            \"3\": class_3\n",
    "        }\n",
    "        \n",
    "        g1 = lambda x: g(x, means[mapping[\"1\"]], sigmas[mapping[\"1\"]], len(clusters[mapping[\"1\"]])/x_tot)\n",
    "        g2 = lambda x: g(x, means[mapping[\"2\"]], sigmas[mapping[\"2\"]], len(clusters[mapping[\"2\"]])/x_tot)\n",
    "        g3 = lambda x: g(x, means[mapping[\"3\"]], sigmas[mapping[\"3\"]], len(clusters[mapping[\"3\"]])/x_tot)\n",
    "        \n",
    "        parameters = {\n",
    "            \"means\": means,\n",
    "            \"sigmas\": sigmas,\n",
    "            \"iterations\": it\n",
    "        }\n",
    "        \n",
    "        return g1, g2, g3, parameters, clusters\n",
    "    \n",
    "    if classifier == \"EM\":\n",
    "\n",
    "        sigmas = get_em_sigmas(data)\n",
    "        pi = np.array([(1/3) for i in range(3)])\n",
    "\n",
    "        means, sigmas, gammas, pi, it = em(samples, means, sigmas, pi)\n",
    "\n",
    "        class_1 = np.argmin([norm(mean_1 - means[k]) for k in range(3)])\n",
    "        class_2 = np.argmin([norm(mean_2 - means[k]) for k in range(3)])\n",
    "        class_3 = np.argmin([norm(mean_3 - means[k]) for k in range(3)])\n",
    "\n",
    "        # create the mapping\n",
    "        mapping = {\n",
    "            \"1\": class_1,\n",
    "            \"2\": class_2,\n",
    "            \"3\": class_3\n",
    "        }\n",
    "\n",
    "        # calculate g functions\n",
    "        g1 = lambda x: g(x, means[mapping[\"1\"]], sigmas[mapping[\"1\"]], pi[mapping[\"1\"]])\n",
    "        g2 = lambda x: g(x, means[mapping[\"2\"]], sigmas[mapping[\"2\"]], pi[mapping[\"2\"]])\n",
    "        g3 = lambda x: g(x, means[mapping[\"3\"]], sigmas[mapping[\"3\"]], pi[mapping[\"3\"]])\n",
    "        \n",
    "        parameters = {\n",
    "            \"means\": means,\n",
    "            \"sigmas\": sigmas,\n",
    "            \"pi\": pi,\n",
    "            \"gammas\": gammas,\n",
    "            \"iterations\": it\n",
    "        }\n",
    "        \n",
    "        return g1, g2, g3, parameters, []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(g1, g2, g3, datasets):\n",
    "    \n",
    "    positive_1 = clasify(datasets[\"test\"][\"1\"], g1, g2, g3, \"1\")\n",
    "    positive_2 = clasify(datasets[\"test\"][\"2\"], g1, g2, g3, \"2\")\n",
    "    positive_3 = clasify(datasets[\"test\"][\"3\"], g1, g2, g3, \"3\")\n",
    "\n",
    "    print(\"Test for class 1 - accuracy: {}\".format(positive_1/len(datasets[\"test\"][\"1\"])))\n",
    "    print(\"Test for class 2 - accuracy: {}\".format(positive_2/len(datasets[\"test\"][\"2\"])))\n",
    "    print(\"Test for class 3 - accuracy: {}\".format(positive_3/len(datasets[\"test\"][\"3\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples, datasets = parse(\"a.txt\", \"o.txt\", \"u.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "g1, g2, g3, parameters, clus = learn(samples, datasets, \"k-means\", \"random\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test for class 1 - accuracy: 1.0\n",
      "Test for class 2 - accuracy: 0.8666666666666667\n",
      "Test for class 3 - accuracy: 0.8666666666666667\n"
     ]
    }
   ],
   "source": [
    "classify(g1, g2, g3, datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot(samples, parameters)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
